{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The simplest way is use tensorflow keras API..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from imp import reload\n",
    "import pickle\n",
    "from string import Template\n",
    "import keras\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "reload(plt)\n",
    "import matplotlib.pyplot as plt\n",
    "reload(plt)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import scipy.io\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.layers.pooling import MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.metrics.classification import confusion_matrix\n",
    "from keras.callbacks import TensorBoard\n",
    "from io import StringIO\n",
    "import base64\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_defect_types = (\"HB\",\"HH\")\n",
    "sensors = tuple([u'BL_AE', u'BR_AE', u'BL_Y', u'BR_Mic', u'BL_Mic', u'MR_Y', u'MR_Z', u'BR_Y', u'BR_X', u'BR_Z', u'MR_X', u'BL_Z'])\n",
    "br_sensors = tuple([u'BR_Mic'])\n",
    "speeds = tuple([u'300', u'2460', u'2580', u'1620', u'2700', u'1740', u'1860', u'1020', u'1980', u'1140', u'2100', u'1260', u'2220', u'1380', u'2340'])\n",
    "mass_locations = tuple([u'LM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(sensors, mass_locations, speeds, defect_types, runs=(1, 2), domain='AD', window_size=360):\n",
    "    def load_mat(file_name, columns):\n",
    "        # the way the data is encoded in the matlab file is a bit cryptic and the following\n",
    "        # lines have been derived by reverse engineering the data structure\n",
    "        mat = scipy.io.loadmat(file_name)['data'][0][0]\n",
    "        data = {c[0]: a.flatten() for c, a in zip(mat.dtype.descr, mat) if c[0] in columns}\n",
    "        return np.stack([data[c] for c in columns]).T\n",
    "    data_path = 'C:/Users/ty8btn/Desktop/Python Notebook/LMS_predictive_data/dataset/'\n",
    "    xs = []\n",
    "    ys = []\n",
    "    # run through the matlab files to load accring to arguments\n",
    "    for defect_type_num, defect_type in enumerate(defect_types):\n",
    "        for mass_location in mass_locations:\n",
    "            for speed in speeds:\n",
    "                for run in runs:\n",
    "                    file_name = '%s_%s_S%s_%s_%s.mat' % (str(defect_type), mass_location, speed, run, domain)\n",
    "                    ##print(\"loading \" + file_name)\n",
    "                    x = load_mat(data_path + file_name, columns=sensors)\n",
    "                    x = x[:int(len(x) / window_size) * window_size]\n",
    "                    xs.append(x)\n",
    "                    ys.append(np.array([defect_type_num] * int(len(x) / window_size)))\n",
    "    # concatenate and normalize data\n",
    "    x = np.concatenate(xs)\n",
    "    x = x / x.var(axis=0)\n",
    "    x = x - x.mean(axis=0)\n",
    "    x = x.reshape(-1, window_size, x.shape[-1])\n",
    "    y = to_categorical(np.concatenate(ys).flatten())\n",
    "    # shuffle patterns to remove temporal correlation\n",
    "    p = np.random.permutation(len(x))\n",
    "    x = x[p]\n",
    "    y = y[p]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = load_data(**{'sensors': sensors, 'speeds': speeds, 'defect_types': all_defect_types, 'mass_locations': mass_locations})\n",
    "l = len(train_data[0])\n",
    "val_data = (train_data[0][int(l * 0.9):], train_data[1][int(l * 0.9):])\n",
    "train_data = (train_data[0][:int(l * 0.9)], train_data[1][:int(l * 0.9)])\n",
    "    \n",
    "x, y = train_data\n",
    "all_defect_types = (\"HB\", \"HH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n"
     ]
    }
   ],
   "source": [
    "conv1 = tf.layers.conv1d(inputs =tf.to_float(tf.convert_to_tensor(train_data[0])) , filters = 50, kernel_size = 30, padding = \"same\",\n",
    "                         kernel_regularizer = tf.contrib.layers.l2_regularizer(0.0), activation = tf.nn.relu) \n",
    "pool1 = tf.layers.max_pooling1d(inputs = conv1, pool_size =10, strides = 1, padding = \"valid\")\n",
    "    \n",
    "conv2 = tf.layers.conv1d(inputs = pool1, filters = 10, kernel_size = 15, padding = \"same\",\n",
    "                             kernel_regularizer = tf.contrib.layers.l2_regularizer(0.0), activation = tf.nn.relu)\n",
    "    \n",
    "pool2 = tf.layers.max_pooling1d(inputs = conv2, pool_size = 3, strides = 1, padding = \"valid\")\n",
    "\n",
    "flatten = tf.contrib.layers.flatten(inputs = pool2)  # in the future, there will have tf.layers.flatten, \n",
    "    #and this tf.contrib.layers.flatten will be deprecated\n",
    "    \n",
    "dense1 = tf.layers.dense(inputs = flatten, units = 25, activation = tf.tanh,\n",
    "                             kernel_regularizer = tf.contrib.layers.l2_regularizer(0.0))\n",
    "dense2 = tf.layers.dense(inputs = dense1, units = y.shape[1], activation = tf.nn.softmax) # binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### lose and train\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = dense2, labels = y))\n",
    "#### use adam optimizer to train\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "# compute the accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(dense2,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_mat = train_data[0]\n",
    "train_label = train_data[1]\n",
    "val_mat = val_data[0]\n",
    "val_label = val_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs_completed = 0\n",
    "index_in_epoch = 0\n",
    "num_examples = train_data[0].shape[0]\n",
    "\n",
    "# serve data by batches\n",
    "def next_batch(batch_size):\n",
    "    \n",
    "    global train_mat\n",
    "    global train_label\n",
    "    global index_in_epoch\n",
    "    global epochs_completed\n",
    "    \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    # when all trainig data have been already used, it is reorder randomly    \n",
    "    if index_in_epoch > num_examples:\n",
    "        # finished epoch\n",
    "        epochs_completed += 1\n",
    "        # shuffle the data\n",
    "        perm = np.arange(num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        train_mat = train_mat[perm]\n",
    "        train_label = train_label[perm]\n",
    "        # start next epoch\n",
    "        start = 0\n",
    "        index_in_epoch = batch_size\n",
    "        assert batch_size <= num_examples\n",
    "    end = index_in_epoch\n",
    "    return train_mat[start:end], train_label[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start TensorFlow session\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation variables\n",
    "train_accuracies = []\n",
    "validation_accuracies = []\n",
    "x_range = []\n",
    "\n",
    "display_step=1\n",
    "BATCH_SIZE = 50\n",
    "TRAINING_ITERATIONS = 100\n",
    "\n",
    "\n",
    "x = tf.placeholder('float', shape=[50, 360, 12])\n",
    "y_ = tf.placeholder('float', shape=[50, 2])\n",
    "\n",
    "\n",
    "for i in range(TRAINING_ITERATIONS):\n",
    "\n",
    "    #get new batch\n",
    "    batch_xs, batch_ys = next_batch(BATCH_SIZE)        \n",
    "\n",
    "    # check progress on every 1st,2nd,...,10th,20th,...,100th... step\n",
    "    if i%display_step == 0 or (i+1) == TRAINING_ITERATIONS:\n",
    "        \n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_xs, \n",
    "                                                  y_: batch_ys})       \n",
    "        if(VALIDATION_SIZE):\n",
    "            validation_accuracy = accuracy.eval(feed_dict={ x: val_mat[0:BATCH_SIZE], y_: val_lable[0:BATCH_SIZE]})                                  \n",
    "            print('training_accuracy / validation_accuracy => %.2f / %.2f for step %d'%(train_accuracy, validation_accuracy, i))\n",
    "            \n",
    "            validation_accuracies.append(validation_accuracy)\n",
    "            \n",
    "        else:\n",
    "             print('training_accuracy => %.4f for step %d'%(train_accuracy, i))\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        x_range.append(i)\n",
    "        \n",
    "        # increase display_step\n",
    "        if i%(display_step*10) == 0 and i:\n",
    "            display_step *= 10\n",
    "    # train on batch\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys, keep_prob: DROPOUT})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##all_train\n",
    "all_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       ..., \n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([train_data[1],val_data[1]],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48628, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1].shape\n",
    "##val_data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(train_data_spec, val_data_spec=None,model_name=\"model\"):\n",
    "    train_data = load_data(**train_data_spec)\n",
    "    l = len(train_data[0])\n",
    "    val_data = (train_data[0][int(l * 0.9):], train_data[1][int(l * 0.9):])\n",
    "    train_data = (train_data[0][:int(l * 0.9)], train_data[1][:int(l * 0.9)])\n",
    "    x, y = train_data\n",
    "    all_defect_types = (\"HB\", \"HH\")\n",
    "    print(\"creating model\")\n",
    "    model = Sequential([\n",
    "        Conv1D(50, 30, padding='same', input_shape=x.shape[1:], kernel_regularizer=l2()), # here ,kernal_regularizer = l2() \n",
    "        # does not make sense, cuz the default is 0.0 means no penalty\n",
    "        MaxPooling1D(pool_size=10, strides=None, padding='valid'), #??? strides = None???\n",
    "        Activation('relu'),\n",
    "        Conv1D(10, 15, padding='same', kernel_regularizer=l2()),\n",
    "        MaxPooling1D(pool_size=3, strides=None, padding='valid'),\n",
    "        Activation('relu'),\n",
    "        Flatten(),\n",
    "        Dense(25, kernel_regularizer=l2()),\n",
    "        Activation('tanh'),\n",
    "        Dense(y.shape[1]),\n",
    "        Activation('softmax')\n",
    "    ]) \n",
    "    print(\"compiling model\")\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    print(\"fitting model\")\n",
    "    callback_history = model.fit(x, y, validation_data=val_data, epochs=2, batch_size=64,verbose=2,\n",
    "                                 callbacks=[TensorBoard(log_dir='C:/Users/ty8btn/Desktop/Python Notebook/lms/tensorboard', histogram_freq=0, write_graph=True),\n",
    "                                            keras.callbacks.BaseLogger(),\n",
    "                                            keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, mode='max'),\n",
    "                                            keras.callbacks.ModelCheckpoint(\"C:/Users/ty8btn/Desktop/Python Notebook/lms/models/%s_weights.hdf5\" % model_name,\n",
    "                                                                            monitor='val_acc',\n",
    "                                                                            save_best_only=True, mode='max')])\n",
    "    print(\"saving model\")\n",
    "    model.save(\"C:/Users/ty8btn/Desktop/Python Notebook/lms/models/%s_architecture.config\" % model_name)\n",
    "    # restore best model\n",
    "    model.load_weights(\"C:/Users/ty8btn/Desktop/Python Notebook/lms/models/%s_weights.hdf5\" % model_name)\n",
    "    # calculate confusion matrix\n",
    "    yt = callback_history.validation_data[1].argmax(axis=1)\n",
    "    print(\"Generating class predictions\")\n",
    "    yp = model.predict_classes(callback_history.validation_data[0], verbose = 0)\n",
    "    cm = confusion_matrix(yt, yp)\n",
    "    cm = ((100.0 * cm.T) / cm.sum(axis=1)).T\n",
    "    cm_df = pd.DataFrame(cm)\n",
    "    cm_df.columns = all_defect_types\n",
    "    cm_df.index = all_defect_types\n",
    "    num_val = len(val_data[0])\n",
    "    max_val_acc = max(callback_history.history['val_acc'])\n",
    "    num_train = len(x)\n",
    "    confusion = cm_df.to_html()\n",
    "    learning_curve_acc = callback_history.history['acc']\n",
    "    learning_curve_val_acc = callback_history.history['val_acc']\n",
    "    # package all information needed for reporting\n",
    "    info = {'model_name': model_name, 'learning_curve_val_acc': learning_curve_val_acc,\n",
    "            'learning_curve_acc': learning_curve_acc, 'num_train': num_train, 'confusion': confusion,\n",
    "            'num_val': num_val, 'max_val_acc': max_val_acc}\n",
    "    return info\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating model\n",
      "compiling model\n",
      "fitting model\n",
      "Train on 48628 samples, validate on 5404 samples\n",
      "Epoch 1/2\n",
      "43s - loss: 0.1873 - acc: 0.9900 - val_loss: 0.0293 - val_acc: 0.9998\n",
      "Epoch 2/2\n",
      "43s - loss: 0.0448 - acc: 0.9948 - val_loss: 0.0201 - val_acc: 1.0000\n",
      "saving model\n",
      "Generating class predictions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'confusion': '<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>HB</th>\\n      <th>HH</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>HB</th>\\n      <td>100.0</td>\\n      <td>0.0</td>\\n    </tr>\\n    <tr>\\n      <th>HH</th>\\n      <td>0.0</td>\\n      <td>100.0</td>\\n    </tr>\\n  </tbody>\\n</table>',\n",
       " 'learning_curve_acc': [0.98998519371555482, 0.99479723616023685],\n",
       " 'learning_curve_val_acc': [0.9998149518874907, 1.0],\n",
       " 'max_val_acc': 1.0,\n",
       " 'model_name': 'test',\n",
       " 'num_train': 48628,\n",
       " 'num_val': 5404}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(model_name = \"test\",train_data_spec=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
