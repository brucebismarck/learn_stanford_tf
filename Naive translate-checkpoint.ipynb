{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use tensorflow.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from string import Template\n",
    "\n",
    "from tensorflow.contrib import keras as keras\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import scipy.io\n",
    "from tensorflow.contrib.keras.python.keras.layers import Conv1D\n",
    "from tensorflow.contrib.keras.python.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.contrib.keras.python.keras.layers import MaxPooling1D\n",
    "from tensorflow.contrib.keras.python.keras.models import Sequential\n",
    "from tensorflow.contrib.keras.python.keras.regularizers import l2\n",
    "from tensorflow.contrib.keras.python.keras.utils import to_categorical\n",
    "from sklearn.metrics.classification import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_defect_types = (\"HB\",\"HH\")\n",
    "sensors = tuple([u'BL_AE', u'BR_AE', u'BL_Y', u'BR_Mic', u'BL_Mic', u'MR_Y', u'MR_Z', u'BR_Y', u'BR_X', u'BR_Z', u'MR_X', u'BL_Z'])\n",
    "br_sensors = tuple([u'BR_Mic'])\n",
    "speeds = tuple([u'300', u'2460', u'2580', u'1620', u'2700', u'1740', u'1860', u'1020', u'1980', u'1140', u'2100', u'1260', u'2220', u'1380', u'2340'])\n",
    "mass_locations = tuple([u'LM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(sensors, mass_locations, speeds, defect_types, runs=(1, 2), domain='AD', window_size=360):\n",
    "    def load_mat(file_name, columns):\n",
    "        # the way the data is encoded in the matlab file is a bit cryptic and the following\n",
    "        # lines have been derived by reverse engineering the data structure\n",
    "        mat = scipy.io.loadmat(file_name)['data'][0][0]\n",
    "        data = {c[0]: a.flatten() for c, a in zip(mat.dtype.descr, mat) if c[0] in columns}\n",
    "        return np.stack([data[c] for c in columns]).T\n",
    "    data_path = 'C:/Users/ty8btn/Desktop/Python Notebook/LMS_predictive_data/dataset/'\n",
    "    xs = []\n",
    "    ys = []\n",
    "    # run through the matlab files to load accring to arguments\n",
    "    for defect_type_num, defect_type in enumerate(defect_types):\n",
    "        for mass_location in mass_locations:\n",
    "            for speed in speeds:\n",
    "                for run in runs:\n",
    "                    file_name = '%s_%s_S%s_%s_%s.mat' % (str(defect_type), mass_location, speed, run, domain)\n",
    "                    ##print(\"loading \" + file_name)\n",
    "                    x = load_mat(data_path + file_name, columns=sensors)\n",
    "                    x = x[:int(len(x) / window_size) * window_size]\n",
    "                    xs.append(x)\n",
    "                    ys.append(np.array([defect_type_num] * int(len(x) / window_size)))\n",
    "    # concatenate and normalize data\n",
    "    x = np.concatenate(xs)\n",
    "    x = x / x.var(axis=0)\n",
    "    x = x - x.mean(axis=0)\n",
    "    x = x.reshape(-1, window_size, x.shape[-1])\n",
    "    y = to_categorical(np.concatenate(ys).flatten())\n",
    "    # shuffle patterns to remove temporal correlation\n",
    "    p = np.random.permutation(len(x))\n",
    "    x = x[p]\n",
    "    y = y[p]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(train_data, val_data, model_name=\"model\"):\n",
    "    x, y = train_data\n",
    "    print(\"creating model\")\n",
    "    model = Sequential([\n",
    "#        Conv1D(50, 30, input_shape=x.shape[1:], kernel_regularizer=l2()),\n",
    "        Conv1D(50, 30, padding='same', input_shape=x.shape[1:], kernel_regularizer=l2()),\n",
    "#        MaxPooling1D(pool_size=10, strides=None),\n",
    "        MaxPooling1D(pool_size=10, strides=None, padding='valid'),\n",
    "        Activation('relu'),\n",
    "#        Conv1D(10, 15, kernel_regularizer=l2()),\n",
    "        Conv1D(10, 15, padding='same', kernel_regularizer=l2()),\n",
    "#        MaxPooling1D(pool_size=3, strides=None),\n",
    "        MaxPooling1D(pool_size=3, strides=None, padding='valid'),\n",
    "        Activation('relu'),\n",
    "        Flatten(),\n",
    "        Dense(25, kernel_regularizer=l2()),\n",
    "        Activation('tanh'),\n",
    "        Dense(y.shape[1]),\n",
    "        Activation('softmax')\n",
    "    ])\n",
    "\n",
    "    print(\"compiling model\")\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "    print(\"fitting model\")\n",
    "    callback_history = model.fit(x, y, validation_data=val_data, epochs=50, batch_size=64,\n",
    "                                 callbacks=[keras.callbacks.BaseLogger(),\n",
    "                                            keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, mode='max'),\n",
    "                                            keras.callbacks.ModelCheckpoint(\"C:/Users/ty8btn/Desktop/Python Notebook/lms/results/models/%s_weights.hdf5\" % model_name,\n",
    "                                                                            monitor='val_acc', verbose=2,\n",
    "                                                                            save_best_only=True, mode='max')])\n",
    "\n",
    "    print(\"saving model\")\n",
    "    model.save(\"C:/Users/ty8btn/Desktop/Python Notebook/lms/results/models/%s_architecture.config\" % model_name)\n",
    "\n",
    "    # restore best model\n",
    "    model.load_weights(\"C:/Users/ty8btn/Desktop/Python Notebook/lms/results/models/%s_weights.hdf5\" % model_name)\n",
    "\n",
    "    # calculate confusion matrix\n",
    "    yt = callback_history.validation_data[1].argmax(axis=1)\n",
    "    yp = model.predict_classes(callback_history.validation_data[0])\n",
    "    cm = confusion_matrix(yt, yp)\n",
    "    cm = ((100.0 * cm.T) / cm.sum(axis=1)).T\n",
    "    cm_df = pd.DataFrame(cm)\n",
    "    cm_df.columns = all_defect_types\n",
    "    cm_df.index = all_defect_types\n",
    "\n",
    "    num_val = len(val_data[0])\n",
    "    max_val_acc = max(callback_history.history['val_acc'])\n",
    "    num_train = len(x)\n",
    "    confusion = cm_df.to_html()\n",
    "    learning_curve_acc = callback_history.history['acc']\n",
    "    learning_curve_val_acc = callback_history.history['val_acc']\n",
    "\n",
    "    # package all information needed for reporting\n",
    "    info = {'model_name': model_name, 'learning_curve_val_acc': learning_curve_val_acc,\n",
    "            'learning_curve_acc': learning_curve_acc, 'num_train': num_train, 'confusion': confusion,\n",
    "            'num_val': num_val, 'max_val_acc': max_val_acc}\n",
    "\n",
    "    return info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_experiment(model_name, train_data_spec, val_data_spec=None):\n",
    "    train_data = load_data(**train_data_spec)\n",
    "\n",
    "    # if no validation data spec, use same spec as training data and split training data accordingly\n",
    "    if val_data_spec is None:\n",
    "        l = len(train_data[0])\n",
    "        val_data = (train_data[0][int(l * 0.9):], train_data[1][int(l * 0.9):])\n",
    "        train_data = (train_data[0][:int(l * 0.9)], train_data[1][:int(l * 0.9)])\n",
    "    else:\n",
    "        val_data = load_data(**val_data_spec)\n",
    "\n",
    "    #train and get info for report\n",
    "    info = train_model(train_data, val_data, model_name=model_name)\n",
    "    info.update({'train_data_spec': train_data_spec, 'val_data_spec': val_data_spec})\n",
    "\n",
    "    # save info just in case we want to change report format later (without rerunning al experiments)\n",
    "    with open('C:/Users/ty8btn/Desktop/Python Notebook/lms/models/' + model_name + \".pkl\", 'wb') as f:\n",
    "        pickle.dump(info, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating model\n",
      "compiling model\n",
      "fitting model\n",
      "Train on 48628 samples, validate on 5404 samples\n",
      "Epoch 1/50\n",
      "48576/48628 [============================>.] - ETA: 0s - loss: 0.1822 - acc: 0.9901Epoch 00000: val_acc improved from -inf to 0.99981, saving model to C:/Users/ty8btn/Desktop/Python Notebook/lms/results/models/test_weights.hdf5\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to create file (Unable to open file: name = 'c:/users/ty8btn/desktop/python notebook/lms/results/models/test_weights.hdf5', errno = 2, error message = 'no such file or directory', flags = 13, o_flags = 302)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d00467f18ff9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun_experiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"test\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'sensors'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'speeds'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mspeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'defect_types'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mall_defect_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mass_locations'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmass_locations\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-f0202ed46d37>\u001b[0m in \u001b[0;36mrun_experiment\u001b[1;34m(model_name, train_data_spec, val_data_spec)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m#train and get info for report\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0minfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'train_data_spec'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_data_spec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'val_data_spec'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mval_data_spec\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-bf67f97e583c>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(train_data, val_data, model_name)\u001b[0m\n\u001b[0;32m     29\u001b[0m                                             keras.callbacks.ModelCheckpoint(\"C:/Users/ty8btn/Desktop/Python Notebook/lms/results/models/%s_weights.hdf5\" % model_name,\n\u001b[0;32m     30\u001b[0m                                                                             \u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m                                                                             save_best_only=True, mode='max')])\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"saving model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ty8btn\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\keras\\python\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[0;32m    842\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m         \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 844\u001b[1;33m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ty8btn\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\keras\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[0;32m   1493\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1494\u001b[0m         \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1495\u001b[1;33m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1497\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ty8btn\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\keras\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1156\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1157\u001b[0m               \u001b[0mepoch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1158\u001b[1;33m       \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1159\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1160\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ty8btn\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\keras\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ty8btn\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\keras\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    420\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ty8btn\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\keras\\python\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[0;32m   2213\u001b[0m     \"\"\"\n\u001b[0;32m   2214\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msave_model\u001b[0m  \u001b[1;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2215\u001b[1;33m     \u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2217\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ty8btn\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\keras\\python\\keras\\models.py\u001b[0m in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[0;32m    118\u001b[0m       \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m   \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m   \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'keras_version'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras_version\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m   \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'backend'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ty8btn\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m                 \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ty8btn\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;31m# Open in append mode (read/write).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (C:\\Minonda\\conda-bld\\h5py_1490029446037\\work\\h5py\\_objects.c:2867)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (C:\\Minonda\\conda-bld\\h5py_1490029446037\\work\\h5py\\_objects.c:2825)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create (C:\\Minonda\\conda-bld\\h5py_1490029446037\\work\\h5py\\h5f.c:2307)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to create file (Unable to open file: name = 'c:/users/ty8btn/desktop/python notebook/lms/results/models/test_weights.hdf5', errno = 2, error message = 'no such file or directory', flags = 13, o_flags = 302)"
     ]
    }
   ],
   "source": [
    "run_experiment(model_name=\"test\", train_data_spec = {'sensors': sensors, 'speeds': speeds, 'defect_types': all_defect_types, 'mass_locations': mass_locations})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
